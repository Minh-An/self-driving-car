{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Lab\n",
    "\n",
    "![](image/lenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('.', reshape=False)\n",
    "\n",
    "train_features, train_labels = mnist.train.images, mnist.train.labels\n",
    "validation_features, validation_labels = mnist.validation.images, mnist.validation.labels\n",
    "test_features, test_labels = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST data in 28x28x1, but LeNet accepts 32x32xC images, so we need to pad data with zeros, 2 for each side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "train_features = np.pad(train_features, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "validation_features = np.pad(validation_features, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "test_features = np.pad(test_features, ((0,0),(2,2),(2,2),(0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0f0f0d0b8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "index = random.randint(0, len(train_features))\n",
    "image = train_features[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(train_labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(input_data):\n",
    "    # arguments for random normal distribution for initializing weights\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    weights = {\n",
    "        'conv1': tf.Variable(tf.truncated_normal([5,5,1,6], mean=mu, stddev=sigma)),\n",
    "        'conv2': tf.Variable(tf.truncated_normal([5,5,6,16], mean=mu, stddev=sigma)),\n",
    "        'fc1': tf.Variable(tf.truncated_normal([400, 120], mean=mu, stddev=sigma)),\n",
    "        'fc2': tf.Variable(tf.truncated_normal([120, 84], mean=mu, stddev=sigma)),\n",
    "        'out': tf.Variable(tf.truncated_normal([84, 10], mean=mu, stddev=sigma))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'conv1': tf.Variable(tf.truncated_normal([6])),\n",
    "        'conv2': tf.Variable(tf.truncated_normal([16])),\n",
    "        'fc1': tf.Variable(tf.truncated_normal([120])),\n",
    "        'fc2': tf.Variable(tf.truncated_normal([84])),\n",
    "        'out': tf.Variable(tf.truncated_normal([10]))\n",
    "    }\n",
    "    \n",
    "    pool_size = [1,2,2,1]\n",
    "    conv_strides = [1,1,1,1]\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    Z1 = tf.nn.conv2d(input_data, weights['conv1'], strides=conv_strides, padding='VALID')\n",
    "    Z1 = tf.nn.bias_add(Z1, biases['conv1'])\n",
    "\n",
    "    # Activation.\n",
    "    A1 = tf.nn.sigmoid(Z1)\n",
    "\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    P1 = tf.nn.max_pool(A1, ksize=pool_size, strides=pool_size, padding='SAME')\n",
    "\n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    Z2 = tf.nn.conv2d(P1, weights['conv2'], strides=conv_strides, padding='VALID')\n",
    "    Z2 = tf.nn.bias_add(Z2, biases['conv2'])\n",
    "    \n",
    "    # Activation.\n",
    "    A2 = tf.nn.sigmoid(Z2)\n",
    "\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    P2 = tf.nn.max_pool(A2, ksize=pool_size, strides=pool_size, padding='SAME')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    flat = flatten(P2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    FC1 = tf.add(tf.matmul(flat, weights['fc1']), biases['fc1'])\n",
    "    \n",
    "    # Activation.\n",
    "    A3 = tf.nn.sigmoid(FC1)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    FC2 = tf.add(tf.matmul(A3, weights['fc2']), biases['fc2'])\n",
    "    \n",
    "    # Activation.\n",
    "    A4 = tf.nn.sigmoid(FC2)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    logits = tf.add(tf.matmul(A4, weights['out']), biases['out'])\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32,32,1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .001\n",
    "\n",
    "logits = LeNet(x)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate With Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 Validation Accuracy = 0.870\n",
      "EPOCH 2 Validation Accuracy = 0.945\n",
      "EPOCH 3 Validation Accuracy = 0.964\n",
      "EPOCH 4 Validation Accuracy = 0.972\n",
      "EPOCH 5 Validation Accuracy = 0.976\n",
      "EPOCH 6 Validation Accuracy = 0.977\n",
      "EPOCH 7 Validation Accuracy = 0.981\n",
      "EPOCH 8 Validation Accuracy = 0.981\n",
      "EPOCH 9 Validation Accuracy = 0.981\n",
      "EPOCH 10 Validation Accuracy = 0.982\n",
      "EPOCH 11 Validation Accuracy = 0.980\n",
      "EPOCH 12 Validation Accuracy = 0.985\n",
      "EPOCH 13 Validation Accuracy = 0.986\n",
      "EPOCH 14 Validation Accuracy = 0.988\n",
      "EPOCH 15 Validation Accuracy = 0.984\n",
      "EPOCH 16 Validation Accuracy = 0.988\n",
      "EPOCH 17 Validation Accuracy = 0.987\n",
      "EPOCH 18 Validation Accuracy = 0.986\n",
      "EPOCH 19 Validation Accuracy = 0.987\n",
      "EPOCH 20 Validation Accuracy = 0.989\n",
      "EPOCH 21 Validation Accuracy = 0.988\n",
      "EPOCH 22 Validation Accuracy = 0.987\n",
      "EPOCH 23 Validation Accuracy = 0.987\n",
      "EPOCH 24 Validation Accuracy = 0.988\n",
      "EPOCH 25 Validation Accuracy = 0.989\n",
      "EPOCH 26 Validation Accuracy = 0.990\n",
      "EPOCH 27 Validation Accuracy = 0.989\n",
      "EPOCH 28 Validation Accuracy = 0.989\n",
      "EPOCH 29 Validation Accuracy = 0.989\n",
      "EPOCH 30 Validation Accuracy = 0.988\n",
      "EPOCH 31 Validation Accuracy = 0.991\n",
      "EPOCH 32 Validation Accuracy = 0.990\n",
      "EPOCH 33 Validation Accuracy = 0.989\n",
      "EPOCH 34 Validation Accuracy = 0.991\n",
      "EPOCH 35 Validation Accuracy = 0.991\n",
      "EPOCH 36 Validation Accuracy = 0.991\n",
      "EPOCH 37 Validation Accuracy = 0.990\n",
      "EPOCH 38 Validation Accuracy = 0.988\n",
      "EPOCH 39 Validation Accuracy = 0.990\n",
      "EPOCH 40 Validation Accuracy = 0.989\n",
      "EPOCH 41 Validation Accuracy = 0.990\n",
      "EPOCH 42 Validation Accuracy = 0.990\n",
      "EPOCH 43 Validation Accuracy = 0.990\n",
      "EPOCH 44 Validation Accuracy = 0.990\n",
      "EPOCH 45 Validation Accuracy = 0.988\n",
      "EPOCH 46 Validation Accuracy = 0.988\n",
      "EPOCH 47 Validation Accuracy = 0.991\n",
      "EPOCH 48 Validation Accuracy = 0.990\n",
      "EPOCH 49 Validation Accuracy = 0.990\n",
      "EPOCH 50 Validation Accuracy = 0.989\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_features, train_labels = shuffle(train_features, train_labels)\n",
    "        for batch_start in range(0, len(train_features), BATCH_SIZE):\n",
    "            batch_end = batch_start+BATCH_SIZE\n",
    "            batch_x, batch_y = train_features[batch_start:batch_end], train_labels[batch_start:batch_end]\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y:batch_y})\n",
    "            \n",
    "        valid_acc = sess.run(accuracy, feed_dict={x: validation_features, y: validation_labels})\n",
    "        print(\"EPOCH {} Validation Accuracy = {:.3f}\".format(epoch+1, valid_acc))\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lenet\n",
      "Test Accuracy = 0.987\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={x: test_features, y: test_labels})\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:car2]",
   "language": "python",
   "name": "conda-env-car2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
